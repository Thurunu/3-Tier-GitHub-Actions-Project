# This workflow will do a clean installation of node dependencies, cache/restore them, build the source code and run tests across different versions of node
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-nodejs

name: Node.js CICD

on: # this is the place where we configure at what time actions need to be run, so here we run this action on pushes and pull requests on main
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  compile:
    runs-on: self-hosted # at here we are using a shared host to run action, if we setup a self hosteed runner then we can use its name here
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '22'
        
    - name: Frontend Compilation (Syntax Check)
      run: cd client && find . -name "*.js" -exec node --check {} +

    - name: Backend Compilation (Syntax Check)
      run: cd api && find . -name "*.js" -exec node --check {} +

  gitleaks-scan:
    runs-on: self-hosted
    needs: compile 
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: GitLeaks Setup
        run: |
          gitleaks detect --source ./client --exit-code 1
          gitleaks detect --source ./api --exit-code 1

      - name: Gitleaks Scan
        run: |
          gitleaks detect --source ./client --exit-code 1
          gitleaks detect --source ./api --exit-code 1

  trivy_fs_scan:
    runs-on: self-hosted
    needs: gitleaks-scan
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Trviy vulnerability scanner
        uses: aquasecurity/trivy-action@0.28.0
        with: 
          scan-type: 'fs'           # "fs" = filesystem scan. Scans the current directory (your source code) for vulnerable dependencies & config issues
          scan-ref: '.'             # '.' = scan the repo root (the code you checked out). You could point this to client/ or api/ if you want to scan only part
          format: 'table'           # Output format → nice human-readable table in the Actions logs (other options: json, sarif, template…)
          ignore-unfixed: true      # Ignores vulnerabilities that don’t have a fix available yet (reduces noise, but you might miss awareness of risks)
          vuln-type: 'os,library'   # Scan both OS packages (like apt-installed things) and libraries (like npm packages in node_modules)
          severity: 'CRITICAL,HIGH' # Only report vulnerabilities at these levels. Ignores MEDIUM/LOW to keep signal-to-noise ratio better

  sonar-frontend:
    runs-on: self-hosted
    needs: trivy_fs_scan
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: SonarQube Scan (client)
        uses: sonarsource/sonarqube-scan-action@master
        with:
          projectBaseDir: client # in this repo we have both client and server side codes so we have to mention which directory need to look
          args: >
              -Dsonar.projectKey=myorg_client
              -Dsonar.projectName=myorg_client
              -Dsonar.sources=.
              -Dsonar.exclusions=**/node_modules/**,**/dist/**,**/build/**
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}

  sonar-backend:
    runs-on: self-hosted
    needs: sonar-frontend
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: SonarQube Scan (api)
        uses: sonarsource/sonarqube-scan-action@master
        with:
          projectBaseDir: api # in this repo we have both client and server side codes so we have to mention which directory need to look
          args: >
              -Dsonar.projectKey=myorg_api
              -Dsonar.projectName=myorg_api
              -Dsonar.sources=.
              -Dsonar.exclusions=**/node_modules/**,**/dist/**,**/build/**
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
          
  build_backend_docker_image_and_push:
    runs-on: ubuntu-latest
    needs: sonar-backend
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ vars.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Setup QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push
        uses: docker/build-push-action@v6
        with:
          context: ./api
          file: ./api/Dockerfile
          push: true
          tags: ${{ vars.DOCKERHUB_USERNAME }}/backend:latest
          
  build_frontend_docker_image_and_push:
    runs-on: ubuntu-latest
    needs: sonar-backend
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ vars.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Setup QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push
        uses: docker/build-push-action@v6
        with:
          context: ./client
          file: ./client/Dockerfile
          push: true
          tags: ${{ vars.DOCKERHUB_USERNAME }}/frontend:latest

  trivy-scan-docker-images:
    runs-on: ubuntu-latest
    needs: [build_backend_docker_image_and_push, build_frontend_docker_image_and_push]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Run trivy image scan for backend
        uses: aquasecurity/trivy-action@0.28.0
        with: 
          scan-type: 'image'           # "fs" = filesystem scan. Scans the current directory (your source code) for vulnerable dependencies & config issues
          image-ref: ${{ vars.DOCKERHUB_USERNAME }}/backend:latest            # '.' = scan the repo root (the code you checked out). You could point this to client/ or api/ if you want to scan only part
          format: 'table'           # Output format → nice human-readable table in the Actions logs (other options: json, sarif, template…)
          ignore-unfixed: true      # Ignores vulnerabilities that don’t have a fix available yet (reduces noise, but you might miss awareness of risks)
          vuln-type: 'os,library'   # Scan both OS packages (like apt-installed things) and libraries (like npm packages in node_modules)
          severity: 'CRITICAL,HIGH'
      - name: Run trivy image scan for frontend
        uses: aquasecurity/trivy-action@0.28.0
        with: 
          scan-type: 'image'           # "fs" = filesystem scan. Scans the current directory (your source code) for vulnerable dependencies & config issues
          image-ref: ${{ vars.DOCKERHUB_USERNAME }}/frontend:latest            # '.' = scan the repo root (the code you checked out). You could point this to client/ or api/ if you want to scan only part
          format: 'table'           # Output format → nice human-readable table in the Actions logs (other options: json, sarif, template…)
          ignore-unfixed: true      # Ignores vulnerabilities that don’t have a fix available yet (reduces noise, but you might miss awareness of risks)
          vuln-type: 'os,library'   # Scan both OS packages (like apt-installed things) and libraries (like npm packages in node_modules)
          severity: 'CRITICAL,HIGH'

  setup-k8s-infrastructure:
    runs-on: self-hosted
    needs: trivy-scan-docker-image

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install AWS CLI
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip awscliv2.zip
          sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v5.0.0
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-south-1

      - name: Setup kubernets cli
        uses: azure/setup-kubectl@v4
        with:
          version: latest

      - name: Configure kubeconfig
        run: |
          mkdir -p $HOME/.kube
          echo "${{ secrets.EKS_KUBECONFIG }}" > $HOME/.kube/config
          
      - name: Create namespace for kubectl
        run: kubectl get namespace prod || kubectl create namespace prod

      - name: Install cert-manager
        run: |
          # Check if cert-manager is already installed
          if ! kubectl get namespace cert-manager >/dev/null 2>&1; then
            echo "Installing cert-manager..."
            kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.2/cert-manager.yaml
            echo "Waiting for cert-manager to be ready..."
            kubectl wait --for=condition=Available --timeout=300s deployment/cert-manager -n cert-manager
            kubectl wait --for=condition=Available --timeout=300s deployment/cert-manager-cainjector -n cert-manager
            kubectl wait --for=condition=Available --timeout=300s deployment/cert-manager-webhook -n cert-manager
            
            echo "Waiting for cert-manager webhook to be ready..."
            # Wait for the webhook service to have endpoints
            kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=webhook -n cert-manager --timeout=300s
            
            # Additional wait to ensure webhook is fully operational
            sleep 30
          else
            echo "cert-manager is already installed"
            # Even if already installed, wait for webhook to be ready
            kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=webhook -n cert-manager --timeout=300s
          fi

      - name: Install NGINX Ingress Controller
        run: |
          # Check if nginx-ingress is already installed
          if ! kubectl get namespace ingress-nginx >/dev/null 2>&1; then
            echo "Installing NGINX Ingress Controller..."
            kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/aws/deploy.yaml
            echo "Waiting for NGINX Ingress Controller to be ready..."
            kubectl wait --namespace ingress-nginx --for=condition=ready pod --selector=app.kubernetes.io/component=controller --timeout=300s
          else
            echo "NGINX Ingress Controller is already installed"
          fi

  deploy-to-eks:
    runs-on: self-hosted
    needs: setup-k8s-infrastructure

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v5.0.0
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-south-1

      - name: Setup kubernets cli
        uses: azure/setup-kubectl@v4
        with:
          version: latest

      - name: Configure kubeconfig
        run: |
          mkdir -p $HOME/.kube
          echo "${{ secrets.EKS_KUBECONFIG }}" > $HOME/.kube/config
        
      - name: Deploy application to EKS
        run: |
          # Deploy core application components
          for file in sc.yaml mysql.yaml backend.yaml frontend.yaml; do
            if [ -f "k8s-prod/$file" ]; then
              echo "Applying k8s-prod/$file..."
              kubectl apply -f "k8s-prod/$file"
            else
              echo "Warning: k8s-prod/$file not found."
            fi
          done
          
          # Apply cert-manager ClusterIssuer after cert-manager is ready
          echo "Applying cert-manager ClusterIssuer..."
          # Retry mechanism for ClusterIssuer application
          for i in {1..5}; do
            if kubectl apply -f k8s-prod/ci.yaml; then
              echo "ClusterIssuer applied successfully"
              break
            else
              echo "Attempt $i failed, waiting 30 seconds before retry..."
              sleep 30
            fi
          done
          
          # Apply ingress last
          echo "Applying ingress..."
          kubectl apply -f k8s-prod/ingress.yaml
          
          # Verify deployment status
          echo "Checking deployment status..."
          kubectl get pods -n prod
          kubectl get services -n prod
          kubectl get ingress -n prod

